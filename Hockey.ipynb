{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are scraping this : \n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/?page_num=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Steps:**\n",
    "\n",
    "1. Import the necessary packages:\n",
    "   - `import requests` \n",
    "   - `from bs4 import BeautifulSoup` \n",
    "2. Use `requests.get()` to retrieve the necessary details of the page to be scraped.\n",
    "   \n",
    "   - `page = requests.get(\"page_url\")` where `page_url` is the URL of page to be scraped.\n",
    "   - can check `page.status_code` to know if the connection is fine ie `status_code = 200` else `status_code = 404` for some error such as if the page doesnt exist.\n",
    "3. Use `BeautifulSoup` to create a soup object as `page_soup = BeautifulSoup(markup = page.content, features= \"html.parser\")` or simply, `page_soup = BeautifulSoup(page.content, \"html.parser\")`. \n",
    "   \n",
    "4. Alternatively, we can also use `page_soup = BeautifulSoup(page.text, \"html.parser\")`. `page.content` returns `binary` data whereas `page.text` returns `string` data.\n",
    "   \n",
    "   Now, this object holds the `html` codes for the page we are dealing with. \n",
    "   \n",
    "5. Next step is to extract the relevant details via inspecting the code. This can be achieved with the `find`,`findAll`,`findChild` methods in conjunction with `get` and `getText` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing a function to scrape a given page (ie its url). Rather unconventionally, I am also passing the data object to which the scraped data is to be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scraper(page_url: str, team_data: list):\n",
    "    page = requests.get(page_url)\n",
    "    page_soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "\n",
    "    # Extracting Table Data\n",
    "    table_data = page_soup.find_all(\"tr\", class_=\"team\")\n",
    "    # Of course, we have inspected the site's html code inorder to do this.\n",
    "    for row in table_data:\n",
    "        team = {}\n",
    "\n",
    "        name = row.find(\"td\", class_=\"name\").getText(strip=True)\n",
    "        year = int(row.find(\"td\", class_=\"year\").getText(strip=True))\n",
    "        wins = int(row.find(\"td\", class_=\"wins\").getText(strip=True))\n",
    "        losses = int(row.find(\"td\", class_=\"losses\").getText(strip=True))\n",
    "        ot_losses = 0 if not row.find(\"td\", class_=\"ot-losses\").getText(\n",
    "            strip=True) else int(row.find(\"td\", class_=\"ot-losses\").getText(strip=True))\n",
    "        gf = int(row.find(\"td\", class_=\"gf\").getText(strip=True))\n",
    "        ga = int(row.find(\"td\", class_=\"ga\").getText(strip=True))\n",
    "\n",
    "        # upon inspection it was seen that win% consisted of two classes.\n",
    "        # thus we have to take care of it in the following way.\n",
    "        if not row.find(\"td\", class_=\"pct text-success\"):\n",
    "            win_pct = float(\n",
    "                row.find(\"td\", class_=\"pct text-danger\").getText(strip=True))\n",
    "        else:\n",
    "            win_pct = float(\n",
    "                row.find(\"td\", class_=\"pct text-success\").getText(strip=True))\n",
    "        # similarly, as above, there were two classes for goal-difference (gd)\n",
    "        if not row.find(\"td\", class_=\"diff text-success\"):\n",
    "            gd = int(row.find(\"td\", class_=\"diff text-danger\").getText(strip=True))\n",
    "        else:\n",
    "            gd = int(row.find(\"td\", class_=\"diff text-success\").getText(strip=True))\n",
    "\n",
    "        team[\"name\"] = name\n",
    "        team[\"year\"] = year\n",
    "        team[\"wins\"] = wins\n",
    "        team[\"losses\"] = losses\n",
    "        team[\"ot-losses\"] = ot_losses\n",
    "        team[\"win%\"] = win_pct\n",
    "        team[\"gf\"] = gf\n",
    "        team[\"ga\"] = ga\n",
    "        team[\"gd\"] = gd\n",
    "\n",
    "        team_data.append(team)\n",
    "    print(f\"DONE : {page_url}\")  # just to see progress, entirely optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do a test run with the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25,\n",
       " [{'name': 'Boston Bruins',\n",
       "   'year': 1990,\n",
       "   'wins': 44,\n",
       "   'losses': 24,\n",
       "   'ot-losses': 0,\n",
       "   'win%': 0.55,\n",
       "   'gf': 299,\n",
       "   'ga': 264,\n",
       "   'gd': 35},\n",
       "  {'name': 'Buffalo Sabres',\n",
       "   'year': 1990,\n",
       "   'wins': 31,\n",
       "   'losses': 30,\n",
       "   'ot-losses': 0,\n",
       "   'win%': 0.388,\n",
       "   'gf': 292,\n",
       "   'ga': 278,\n",
       "   'gd': 14},\n",
       "  {'name': 'Calgary Flames',\n",
       "   'year': 1990,\n",
       "   'wins': 46,\n",
       "   'losses': 26,\n",
       "   'ot-losses': 0,\n",
       "   'win%': 0.575,\n",
       "   'gf': 344,\n",
       "   'ga': 263,\n",
       "   'gd': 81}])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "page_scraper(\"https://www.scrapethissite.com/pages/forms/?page_num=1\", data)\n",
    "len(data), data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was just 1 page of data. Now let us see how we can get data from all such pages. We write a function to scrape the entire web site for the team details. Once we go beyond the existing team data, we reach a page which has \"0 items\" displayed. We use that as the break condition. This is far more elegant that manually counting the number of pages and then using that to loop over pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_scraper(team_data: list):\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        base_url = f\"https://www.scrapethissite.com/pages/forms/?page_num={\n",
    "            page_num}\"\n",
    "        tester = requests.get(base_url)\n",
    "        tester_soup = BeautifulSoup(tester.content, \"html.parser\")\n",
    "        validity = tester_soup.find(\"section\").findChild(\n",
    "            \"div\", class_=\"col-md-12\").findChild(\"small\").get_text()\n",
    "\n",
    "        if validity == \"0 items\":\n",
    "            break\n",
    "        page_scraper(base_url, team_data)\n",
    "        page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the scraper to get all the team data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=1\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=2\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=3\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=4\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=5\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=6\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=7\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=8\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=9\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=10\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=11\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=12\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=13\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=14\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=15\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=16\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=17\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=18\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=19\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=20\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=21\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=22\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=23\n",
      "DONE : https://www.scrapethissite.com/pages/forms/?page_num=24\n"
     ]
    }
   ],
   "source": [
    "all_the_data = []\n",
    "site_scraper(all_the_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(582,\n",
       " [{'name': 'Philadelphia Flyers',\n",
       "   'year': 2011,\n",
       "   'wins': 47,\n",
       "   'losses': 26,\n",
       "   'ot-losses': 9,\n",
       "   'win%': 0.573,\n",
       "   'gf': 264,\n",
       "   'ga': 232,\n",
       "   'gd': 32},\n",
       "  {'name': 'Phoenix Coyotes',\n",
       "   'year': 2011,\n",
       "   'wins': 42,\n",
       "   'losses': 27,\n",
       "   'ot-losses': 13,\n",
       "   'win%': 0.512,\n",
       "   'gf': 216,\n",
       "   'ga': 204,\n",
       "   'gd': 12},\n",
       "  {'name': 'Pittsburgh Penguins',\n",
       "   'year': 2011,\n",
       "   'wins': 51,\n",
       "   'losses': 25,\n",
       "   'ot-losses': 6,\n",
       "   'win%': 0.622,\n",
       "   'gf': 282,\n",
       "   'ga': 221,\n",
       "   'gd': 61},\n",
       "  {'name': 'San Jose Sharks',\n",
       "   'year': 2011,\n",
       "   'wins': 43,\n",
       "   'losses': 29,\n",
       "   'ot-losses': 10,\n",
       "   'win%': 0.524,\n",
       "   'gf': 228,\n",
       "   'ga': 210,\n",
       "   'gd': 18},\n",
       "  {'name': 'St. Louis Blues',\n",
       "   'year': 2011,\n",
       "   'wins': 49,\n",
       "   'losses': 22,\n",
       "   'ot-losses': 11,\n",
       "   'win%': 0.598,\n",
       "   'gf': 210,\n",
       "   'ga': 165,\n",
       "   'gd': 45},\n",
       "  {'name': 'Tampa Bay Lightning',\n",
       "   'year': 2011,\n",
       "   'wins': 38,\n",
       "   'losses': 36,\n",
       "   'ot-losses': 8,\n",
       "   'win%': 0.463,\n",
       "   'gf': 235,\n",
       "   'ga': 281,\n",
       "   'gd': -46},\n",
       "  {'name': 'Toronto Maple Leafs',\n",
       "   'year': 2011,\n",
       "   'wins': 35,\n",
       "   'losses': 37,\n",
       "   'ot-losses': 10,\n",
       "   'win%': 0.427,\n",
       "   'gf': 231,\n",
       "   'ga': 264,\n",
       "   'gd': -33},\n",
       "  {'name': 'Vancouver Canucks',\n",
       "   'year': 2011,\n",
       "   'wins': 51,\n",
       "   'losses': 22,\n",
       "   'ot-losses': 9,\n",
       "   'win%': 0.622,\n",
       "   'gf': 249,\n",
       "   'ga': 198,\n",
       "   'gd': 51},\n",
       "  {'name': 'Washington Capitals',\n",
       "   'year': 2011,\n",
       "   'wins': 42,\n",
       "   'losses': 32,\n",
       "   'ot-losses': 8,\n",
       "   'win%': 0.512,\n",
       "   'gf': 222,\n",
       "   'ga': 230,\n",
       "   'gd': -8},\n",
       "  {'name': 'Winnipeg Jets',\n",
       "   'year': 2011,\n",
       "   'wins': 37,\n",
       "   'losses': 35,\n",
       "   'ot-losses': 10,\n",
       "   'win%': 0.451,\n",
       "   'gf': 225,\n",
       "   'ga': 246,\n",
       "   'gd': -21}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_the_data), all_the_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write all this to a file (.txt). For completeness sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"teams.txt\", \"w\") as file:\n",
    "    for data in all_the_data:\n",
    "        file.write(f\"{str(data)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following were some stuff I tested (before I wrote out the final solution above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "0.388\n",
      "0.575\n",
      "0.613\n",
      "0.425\n",
      "0.463\n",
      "0.388\n",
      "0.575\n",
      "0.338\n",
      "0.487\n",
      "0.4\n",
      "0.312\n",
      "0.45\n",
      "0.412\n",
      "0.512\n",
      "0.2\n",
      "0.588\n",
      "0.287\n",
      "0.35\n",
      "0.463\n",
      "0.325\n",
      "0.45\n",
      "0.388\n",
      "0.388\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "test = requests.get(\"https://www.scrapethissite.com/pages/forms/?page_num=1\")\n",
    "test_soup = BeautifulSoup(test.content, \"html.parser\")\n",
    "table_dt = test_soup.find_all(\"tr\", class_=\"team\")\n",
    "\n",
    "for row in table_dt:\n",
    "    if not row.find(\"td\", class_=\"pct text-success\"):\n",
    "        print(row.find(\"td\", class_=\"pct text-danger\").get_text(strip=True))\n",
    "    else:\n",
    "        print(row.find(\"td\", class_=\"pct text-success\").get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 items'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = requests.get(\n",
    "    \"https://www.scrapethissite.com/pages/forms/?page_num=240\")\n",
    "tester_soup = BeautifulSoup(tester.content, \"html.parser\")\n",
    "validity = tester_soup.find(\"section\").findChild(\n",
    "    \"div\", class_=\"col-md-12\").findChild(\"small\").get_text()\n",
    "validity\n",
    "# if tester_soup==\"0 items\":\n",
    "#     print(\"This will work!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
