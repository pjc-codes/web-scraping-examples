{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping -\n",
    "https://www.scrapethissite.com/pages/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Steps:**\n",
    "\n",
    "1. Import the necessary packages:\n",
    "   - `import requests` \n",
    "   - `from bs4 import BeautifulSoup` \n",
    "2. Use `requests.get()` to retrieve the necessary details of the page to be scraped.\n",
    "   \n",
    "   - `page = requests.get(\"page_url\")` where `page_url` is the URL of page to be scraped.\n",
    "   - can check `page.status_code` to know if the connection is fine ie `status_code = 200` else `status_code = 404` for some error such as if the page doesnt exist.\n",
    "3. Use `BeautifulSoup` to create a soup object as `page_soup = BeautifulSoup(markup = page.content, features= \"html.parser\")` or simply, `page_soup = BeautifulSoup(page.content, \"html.parser\")`. \n",
    "   \n",
    "4. Alternatively, we can also use `page_soup = BeautifulSoup(page.text, \"html.parser\")`. `page.content` returns `binary` data whereas `page.text` returns `string` data.\n",
    "   \n",
    "   Now, this object holds the `html` codes for the page we are dealing with. \n",
    "   \n",
    "5. Next step is to extract the relevant details via inspecting the code. This can be achieved with the `find`,`findAll`,`findChild` methods in conjunction with `get` and `getText` methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set up the soup object and prepare for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.scrapethissite.com/pages/simple/\")\n",
    "page_soup = BeautifulSoup(page.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Upon inspecting the HTML code of the page, we can see that the information we seek is within `div` tags having `class=col-md-4 country`. So we will first isolate all those tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-md-4 country\">\n",
       "<h3 class=\"country-name\">\n",
       "<i class=\"flag-icon flag-icon-ad\"></i>\n",
       "                            Andorra\n",
       "                        </h3>\n",
       "<div class=\"country-info\">\n",
       "<strong>Capital:</strong> <span class=\"country-capital\">Andorra la Vella</span><br/>\n",
       "<strong>Population:</strong> <span class=\"country-population\">84000</span><br/>\n",
       "<strong>Area (km<sup>2</sup>):</strong> <span class=\"country-area\">468.0</span><br/>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_data = page_soup.find_all(\"div\", class_=\"col-md-4 country\")\n",
    "countries_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Let us extract the country names by getting them from the `h3` tags within these `div` tags from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,\n",
       " ['Andorra',\n",
       "  'United Arab Emirates',\n",
       "  'Afghanistan',\n",
       "  'Antigua and Barbuda',\n",
       "  'Anguilla',\n",
       "  'Albania',\n",
       "  'Armenia',\n",
       "  'Angola',\n",
       "  'Antarctica',\n",
       "  'Argentina'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_data = page_soup.find_all(\"h3\", class_ = \"country-name\")\n",
    "country_names = []\n",
    "\n",
    "for name in name_data:\n",
    "    country_names.append((name.get_text(strip=True)))\n",
    "\n",
    "len(country_names), country_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Let us now actually extract all the details of each country (name, capital, population and area). We will store it as a dictionary of dictionary. Each country name will act as the key to another dictionary with the demographic details within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_details={}\n",
    "for country in countries_data:\n",
    "    name = country.findChild(\"h3\").get_text(strip=True)\n",
    "    demographics = {}\n",
    "    \n",
    "    capital = country.findChild(\"span\", class_ = \"country-capital\").get_text(strip=True)\n",
    "    population = f\"{int(country.findChild(\"span\", class_ = \"country-population\").get_text(strip=True)):,d}\"\n",
    "    area = f\"{float(country.findChild(\"span\", class_ = \"country-area\").get_text(strip=True)):,.01f}\"\n",
    "\n",
    "    demographics[\"capital\"]=capital\n",
    "    demographics[\"population\"]=population\n",
    "    demographics[\"area (sq.km)\"]=area\n",
    "    country_details[name]=demographics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'capital': 'New Delhi',\n",
       "  'population': '1,173,108,018',\n",
       "  'area (sq.km)': '3,287,590.0'},\n",
       " {'capital': 'Beijing',\n",
       "  'population': '1,330,044,000',\n",
       "  'area (sq.km)': '9,596,960.0'},\n",
       " {'capital': 'Andorra la Vella',\n",
       "  'population': '84,000',\n",
       "  'area (sq.km)': '468.0'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_details[\"India\"], country_details[\"China\"],country_details[\"Andorra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Let's write it all to a text file for posterity. (Note to self: learn how to write to a csv format!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"countries.txt\", \"w\") as file:\n",
    "    for country in country_details:\n",
    "        file.write(f\"{str(country)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
